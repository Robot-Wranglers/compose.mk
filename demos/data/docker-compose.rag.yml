# See the demo docs: https://robot-wranglers.github.io/compose.mk/demos/RAG
#
# NB: GPU support disabled by default; to enable this, use something like this 
# in the ollama_server block. You must also remove the '+cpu' from torch requirement.
#
#   https://docs.docker.com/compose/how-tos/gpu-support/
#   deploy:
#     resources:
#       reservations:
#         devices:
#           - driver: nvidia
#             count: 1
#             capabilities: [gpu]
services:
  ollama_server: &base
    build:
      context: .
      dockerfile_inline: |
        FROM ollama/ollama@sha256:476b956cbe76f22494f08400757ba302fd8ab6573965c09f1e1a66b2a7b0eb77
        RUN apt-get update -qq && apt-get install -qq -y software-properties-common make procps curl
        RUN add-apt-repository ppa:deadsnakes/ppa
        RUN apt-get update -qq && apt-get install -y -qq wget python3.11 python3.11-dev python3.11-venv
        RUN update-alternatives --install \
              /usr/bin/python3 python3 /usr/bin/python3.11 1 \
          && update-alternatives --config python3
        RUN wget https://bootstrap.pypa.io/get-pip.py && python3.11 get-pip.py && rm get-pip.py
        RUN pip install 'torch==2.6.0+cpu' --extra-index-url https://download.pytorch.org/whl/cpu
        RUN pip install --no-cache-dir langchain==0.3.22 langchain-community==0.3.20 
        RUN pip install --no-cache-dir langchain-huggingface==0.1.2 langchain-ollama==0.3.0
        RUN pip install --no-cache-dir ollama==0.4.7 sentence-transformers==4.0.2 faiss-cpu==1.10.0 
        RUN pip install --no-cache-dir tqdm==4.67.1 unstructured[md]==0.17.2
        RUN pip install --no-cache-dir requests==2.32.3 click==8.1.8 rich==14.0.0 accelerate==1.6.0
    entrypoint: ['ollama','serve']
    volumes:
      # NB: this allows model-sharing for any host-installation of ollama if 
      # it's available. Probably wrong for non-Linux and also maybe wrong for 
      # some ollama versions?
      - /usr/share/ollama/.ollama:/root/.ollama
  rag: 
    <<: *base
    working_dir: /workspace
    entrypoint: ["python3.11"]
    links: ['ollama_server'] 
    depends_on: ['ollama_server'] 
    volumes:
      # Share the hosts working directory- this is for document ingestion
      - ${PWD}:/workspace
      # NB: this allows model-sharing for the host-installation of 
      # hugging face if applicable.  Probably wrong for non-Linux and 
      # also maybe wrong for some library versions?
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
    environment: 
      OLLAMA_HOST: ollama_server
      LLM_MODEL: ${LLM_MODEL:-phi3:mini}
      EBD_MODEL_NAME: ${EBD_MODEL_NAME:-all-MiniLM-L6-v2}