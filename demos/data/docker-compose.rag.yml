# See the demo docs: https://robot-wranglers.github.io/compose.mk/demos/RAG
#
# NB: GPU support disabled by default; to enable this, use something like this 
# in the ollama_server block. You must also remove the '+cpu' from torch requirement.
#
#   https://docs.docker.com/compose/how-tos/gpu-support/
#   deploy:
#     resources:
#       reservations:
#         devices:
#           - driver: nvidia
#             count: 1
#             capabilities: [gpu]

# NB: this allows model-sharing for any host-installation of ollama if 
# it's available. Probably wrong for non-Linux and also maybe wrong for 
# some ollama versions?
# Share the hosts working directory- this is for document ingestion
# NB: this allows model-sharing for the host-installation of 
# hugging face if applicable.  Probably wrong for non-Linux and 
# also maybe wrong for some library versions?
services:
  ollama_server: &base
    build:
      context: .
      dockerfile_inline: |
        FROM ollama/ollama@sha256:476b956cbe76f22494f08400757ba302fd8ab6573965c09f1e1a66b2a7b0eb77
    entrypoint: ['ollama','serve']
    working_dir: /workspace
    volumes:
      - /usr/share/ollama/.ollama:/root/.ollama
      - ${PWD}:/workspace
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
    environment:  &base_env
      OLLAMA_HOST: ollama_server
      LLM_MODEL: ${LLM_MODEL:-mistral}
      EBD_MODEL_NAME: ${EBD_MODEL_NAME:-all-MiniLM-L6-v2}
  ollama_python: 
    <<: *base
    entrypoint: ["python3.11"]
    depends_on: ['ollama_server']
    build:
      context: .
      dockerfile_inline: |
        FROM python:3.11-slim-bookworm
        RUN pip install 'torch==2.6.0+cpu' \
          --extra-index-url https://download.pytorch.org/whl/cpu
        RUN pip install --no-cache-dir \
          langchain==0.3.22 langchain-community==0.3.20 \
          langchain-huggingface==0.1.2 langchain-ollama==0.3.0 \
          ollama==0.4.7 sentence-transformers==4.0.2 faiss-cpu==1.10.0 \
          unstructured[md]==0.17.2 click==8.1.8 \
          accelerate==1.6.0 python-magic==0.4.27
        RUN apt-get update -qq && apt-get install -y make procps
  ollama_node:
    <<: *base
    depends_on: ['ollama_server']
    # working_dir: /app
    build:
      context: .
      dockerfile_inline: |
        FROM node:18-slim
        WORKDIR /app
        RUN npm init -y
        RUN npm install @langchain/community @langchain/core langchain
        RUN apt-get update -qq && apt-get install -y make procps
