{% import 'macros.j2' as macros -%}

## Obligatory LLM Thing
<hr style="width:100%;border-bottom:3px solid black;">

This demo allows you to chat with `compose.mk` documentation (or any other files) using a small RAG pipeline implemented with ollama [^1] and langchain.  [^2]

```bash
$ query='describe polyglot capabilities as if you are a pirate' \
    glob='docs/*.md' ./demos/rag.mk ask
```

```markdown
  Ahoy mateys! Arr, in this world o' languages, I be savvy enough to juggle 'em 
  like yer ol' bottle collection— each one perfect for its own purpose. With me 
  trusted compose.mk and some clever splicin', even the most bewilderin tasks 
  gets done easy as pie. Be it leveragin language just right or summonin matrix golf 
  with APL, I command these languages better than any captain commands his ship! 
  Now that be a mighty fine skill to have in this treacherous sea o' code, see?
  
  CITATIONS:
  
  - /workspace/docs/demos/polyglots.md ("Part of the be...")
  - /workspace/docs/style.md ("Explicit polyglot in...")
  - /workspace/docs/but-why.md ("Polyglots Considered...")
```

--------------

The way this demo works is that you specify a corpus with a file pattern glob, and then provide a verb like `summarize`, `ask` or `chat` to select how you want to interact with the corpus.  You can override the models used by the backend, but the default models are `phi3:mini` for the LLM and `all-MiniLM-L6-v2` for the embeddings.  It's worth emphasizing that this demo is **local only**, **CLI-friendly**, and as long as you have `make` and `docker` it will bootstrap itself.  Users can therefore enjoy: *no daemons required, no installation of python dependencies, no API tokens, and no "open this thing in your web browser".*  And the pirates of course.  Pirates describing [polyglots]({{mkdocs.site_relative_url}}/demos/polyglots).

Citations are provided for `ask` interactions and are pretty reasonable, but citations are not implemented for `chat`.  As for the actual answers there are, of course, frequently very strange hallucinations and quality varies.  Consider reading the actual project documentation =)

All support code is included in [the appendix](#appendix-support-code).  Quick links for the moving parts:

1. [Langchain driver](#appendix-langchain-code) *(Python)*
1. [Ollama container](#appendix-ollama-container) plus python reqs *(Docker-Compose)*
1. [Thin wrapper](#orchestration) to combine / orchestrate the pieces above *(compose.mk script)*

### Other Background
<hr style="width:100%;border-bottom:3px solid black;">

Jump ahead if you're waiting for examples and code, but since the appendix is big, let's front-load the discussion.  Besides the fact that the default corpus is this project documentation.. you might be wondering whether this is even *remotely* related to `compose.mk` core competencies.  Fair.  

Python does all the LLM work, but the compose file handles the dependencies, including both Ollama and Python requirements.  The need for at least minimal orchestration comes in because *we may not need the containerized ollama server at all* if one is available locally already, and in that case we just use it.  Meanwhile **if no local ollama server is available,** we start a containerized one and use that instead.  

For a containerized server:

* We forward no ports, 
* Leverage existing models via volumes if they are available on the host, or 
* Download them just in time if necessary, and finally
* Ensure the containerized daemon is shut down after use

By handling (conditional) startup, shutdown, and not actually forwarding ports.. orchestration abstracts the lifecycle stuff and effectively turns a *system* into an application / tool.  

To try and make sure that this idea is more than wishful thinking.. we'll also use `compose.mk` [packaging features](#packaging-it) to wrap all the moving parts and the documents <u>into a single self-extracting executable</u>.

Like the [notebooking demo]({{mkdocs.site_relative_url}}/demos/notebooking), this one is weighty. Poor quality and relatively slow summaries of a few paragraphs in only ~40 markdown files will cost you a ~4gb download of `phi3:mini` if you're not holding it already.  In a lot of ways, you might as well just grep!  GPU acceleration is even explicitly *disabled* too, in the interest of portability[^3].

Nevertheless the PoC is still doing something interesting here and it has potential.  Like the notebooking demo, it takes an "obviously service / system" type of problem and makes it refreshingly "local-first" and [very self-contained](#packaging-it).  Another interesting direction that's not explored is that currently `compose.mk` simply invokes the LLM.. but what happens if the LLM gets access to `compose.mk` via tool-execution?

Without further delay, let's get into it.

### Usage
<hr style="width:100%;border-bottom:3px solid black;">

```bash 
# Ask a question about files that match the given pattern.
$ glob='docs/*.md' query='what is this project about?' ./demos/rag.mk ask

# Summarize files that match the given pattern.
$ glob='docs/*.md' query='what is this project about?' ./demos/rag.mk summarize

# Interactive chat about files that match the given pattern.
$ glob='docs/*.md' ./demos/rag.mk chat

# Slurp any markdown matching the pattern.  
# This is implied by `ask` or `chat`, and no-op if content already indexed.
$ glob='docs/*.md' ./demos/rag.mk ingest

# Ensure given model is available to ollama, downloading if needed.
# Implied by `ingest`, `ask`, and `chat`.
$ model=phi3:mini ./demos/rag.mk init

# Pass everything that's needed and you can do it in one shot
$ model=.. glob=.. query=.. ./demos/rag.mk ask
```
<br/>

### Example Output
<hr style="width:100%;border-bottom:3px solid black;">

Note the glob below is using all the root-documentation markdown, but skips `docs/demos/*.md`.  Using a double-splat like `**.md` for recursive descent in subdirs is also supported.

``` bash 
$ query='why use this project' glob='docs/*.md' ./demos/rag.mk ask
  ⇄  ask // Answering query for corpus: docs/*.md (40 files total) 
  
  Use it for decoupling CI/CD processes from platform lock-in and rapidly 
  incorporating external tools or code into your projects. It fosters an 
  environment that encourages experimentation in component design, prototyping 
  systems, and building console applications with a focus on maintainability and 
  easy collaboration through well-organized repositories.

  CITATIONS:
  
  - docs/overview.md ("Typical use-cases in...")
  - docs/index.md ("Typical use-cases in...")
  - docs/but-why.md ("Motivation & Design...")

```

------------------

Fuzzy search is marginally practical considering citations, but yeah.. might as well grep.

```bash
$ query='which demos are related to justfiles' \
    glob='docs/demos/*.md' ./demos/cmk/rag.cmk ask
  ⇄  ask // Answering query for corpus: docs/demos/*.md (13 files total) 
  
  The Justfile demo is directly related to using and wrapping just-runners with 
  the make recipe system, as well as exposing different interfaces. This 
  demonstrates interoperability between the two systems by showing how compose.mk 
  can incorporate a foreign tool like justfiles within its framework for building 
  container images.
  result: 
  
  CITATIONS:
  - docs/demos/just.md ("Interoperability wit...")
```

------------------

Besides pirate-support, you can ask it to speak to you like a child.  This might be useful for 'explain like I'm a python programmer' or similar, but you'll probably need switch from `phi3:mini` to something bigger (and slower).

```bash 
$ query='whats the best way to get started with compose.mk? explain like i am 5' glob='docs/overview.md' ./demos/cmk/rag.cmk ask
  ⇄  ask // Answering query for corpus: docs/overview.md (1 files total) 
  
  Hey there! Think of compile.mk as a toolbox that helps you build things faster 
  and in more ways than just using regular make commands. Since it is kind of like 
  adding extra fun new tools to your workshop, here are simple steps for beginners on 
  how to start with compose.mk:
  
  1. Get the Toolkit! First off, remember this toolbox is not something you can 
  hold; that means we need a computer and internet connection because everything 
  starts online through GitHub where they share their awesome tools like 
  compile.mk. [snip]
```

<br/>

### Orchestration
<hr style="width:100%;border-bottom:3px solid black;">

The orchestration script isn't *short* but it's very basic.  This is mostly just using `compose.mk` idioms to express very thin wrappers. Wrappers run [the python code](#appendix-langchain-code) inside [the ollama container](#appendix-ollama-container).  Describing dispatch/import idioms is out of scope for this demo since it's all documented in detail elsewhere. See the [scaffolding docs]({{mkdocs.site_relative_url}}/bridge), [dispatch documentation]({{mkdocs.site_relative_url}}/container-dispatch), etc.

To spice it up, this file is written in [CMK-lang]({{mkdocs.site_relative_url}}/compiler) and interpreted by `compose.mk` and transpiled to Makefile, which allows for cleaner syntax in some places.  

**Via: {{macros.repo_link('demos/cmk/rag.cmk', github)}}**

```Makefile
{{open('demos/cmk/rag.cmk','r').read().strip()}}
```

The more interesting and important parts here are really just startup (determining an appropriate value for `OLLAMA_HOST`) and shutdown (`ollama_server.stop`).

### Packaging
<hr style="width:100%;border-bottom:3px solid black;">

As mentioned at the beginning, we want to additionally generate a frozen version of the work so far.  See the [main packaging docs]({{mkdocs.site_relative_url}}/demos/packaging) for more details, but this one-liner is enough to build the executable archive that we need.

```bash 
$ archive='docs demos' bin=agent ./demos/cmk/rag.cmk mk.pkg.root
```

This command creates a new executable called `./agent`, and packages up all of the following:

* The documentation corpus
* The compose file (and thus effectively ollama/langchain dependencies)
* The python script that defines the LLM operations 
* The `rag.cmk` orchestration script, and `compose.mk` itself

To use this self-contained version, we still need to specify arguments and entrypoints, which is similar but slightly different from the [main usage described earlier](#usage).  As before, verbs like `init` and interactive `chat` are also available, and below you can see another example using `ask`.

```bash 
$ query='which features are related to interactivity' query='summarize sections' ./agent  -- ask
  
  The interactive task selector feature mentioned in the context is directly 
  related to interactivity, as it allows users to select tasks interactively 
  within a TUI environment using compose.mk capabilities.
```

This example should now be able to run almost anywhere, including places where ollama is not available, where models are not yet available, and where python is not even installed.  The resulting file is about 7mb because we didn't specify *specific* files earlier and just grabbed the full folders for `docs` and `demos`.  It requires `make` and `docker` and downloads ~4gb of LLM models if you don't have them, but it won't pester you about running a single install command or setting up API keys.  You can download it [here]({{mkdocs.site_relative_url}}/artifacts/agent).

### Appendix: Support Code
<hr style="width:100%;border-bottom:3px solid black;">

{{macros.img_link("gears.jpg", mkdocs, "85%", align='center')}}

#### Appendix: LangChain Code
<hr style="width:100%;border-bottom:3px solid black;">

**Via: {{macros.repo_link('demos/data/rag.py', github)}}**

```python
{{open('demos/data/rag.py','r').read().strip()}}
```

#### Appendix: Ollama Container
<hr style="width:100%;border-bottom:3px solid black;">

**Via: {{macros.repo_link('demos/data/docker-compose.rag.yml', github)}}**

```python
{{open('demos/data/docker-compose.rag.yml','r').read().strip()}}
```

<hr style="width:100%;border-bottom:3px solid black;">

[^1]: https://ollama.com/
[^2]: https://python.langchain.com/docs/introduction/
[^3]: See the compose file for hints about enabling GPUs